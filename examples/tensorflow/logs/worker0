2017-10-17 23:03:53.739441: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-17 23:03:53.739479: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-17 23:03:53.739489: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-10-17 23:03:53.762598: E tensorflow/stream_executor/cuda/cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_UNKNOWN
2017-10-17 23:03:53.762641: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (ip-10-0-0-170): /proc/driver/nvidia/version does not exist
2017-10-17 23:03:53.770471: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> deeplearning-worker1:2222, 1 -> deeplearning-worker2:2222, 2 -> deeplearning-worker3:2222, 3 -> deeplearning-worker4:2222, 4 -> deeplearning-worker5:2222}
2017-10-17 23:03:53.770502: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2230, 1 -> deeplearning-worker2:2230, 2 -> deeplearning-worker3:2230, 3 -> deeplearning-worker4:2230, 4 -> deeplearning-worker5:2230}
2017-10-17 23:03:53.772878: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2230
2017-10-17 23:04:21.579628: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 787171befbe7c35a with config: 

2017-10-17 23:21:37.549167: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
2017-10-17 23:21:37.557573: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
2017-10-17 23:21:37.557878: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
2017-10-17 23:21:37.557951: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
2017-10-17 23:21:37.557961: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
2017-10-17 23:21:37.557981: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
2017-10-17 23:21:37.557992: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
2017-10-17 23:21:37.558000: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
2017-10-17 23:21:37.558016: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
2017-10-17 23:21:37.670643: W tensorflow/core/framework/op_kernel.cc:1158] Invalid argument: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2017-10-17 23:04:29.227185: step 0, loss = 4.67 (30.4 examples/sec; 4.208 sec/batch)
2017-10-17 23:04:32.731367: step 10, loss = 4.59 (370.1 examples/sec; 0.346 sec/batch)
2017-10-17 23:04:36.384829: step 20, loss = 4.45 (330.2 examples/sec; 0.388 sec/batch)
2017-10-17 23:04:39.976966: step 30, loss = 4.44 (375.1 examples/sec; 0.341 sec/batch)
2017-10-17 23:04:43.393818: step 40, loss = 4.37 (368.2 examples/sec; 0.348 sec/batch)
2017-10-17 23:04:46.824510: step 50, loss = 4.32 (371.7 examples/sec; 0.344 sec/batch)
2017-10-17 23:04:50.255776: step 60, loss = 4.28 (377.5 examples/sec; 0.339 sec/batch)
2017-10-17 23:04:53.702444: step 70, loss = 4.32 (374.3 examples/sec; 0.342 sec/batch)
2017-10-17 23:04:57.269704: step 80, loss = 4.33 (362.5 examples/sec; 0.353 sec/batch)
2017-10-17 23:05:00.759002: step 90, loss = 4.01 (366.3 examples/sec; 0.349 sec/batch)
2017-10-17 23:05:04.341429: step 100, loss = 3.96 (362.5 examples/sec; 0.353 sec/batch)
2017-10-17 23:05:07.829558: step 110, loss = 4.10 (371.3 examples/sec; 0.345 sec/batch)
2017-10-17 23:05:11.395773: step 120, loss = 3.87 (305.2 examples/sec; 0.419 sec/batch)
2017-10-17 23:05:14.923090: step 130, loss = 3.79 (369.6 examples/sec; 0.346 sec/batch)
2017-10-17 23:05:18.431254: step 140, loss = 3.99 (366.8 examples/sec; 0.349 sec/batch)
2017-10-17 23:05:21.981095: step 150, loss = 3.47 (369.1 examples/sec; 0.347 sec/batch)
2017-10-17 23:05:25.448815: step 160, loss = 3.35 (370.2 examples/sec; 0.346 sec/batch)
2017-10-17 23:05:30.601642: step 170, loss = 3.41 (371.0 examples/sec; 0.345 sec/batch)
2017-10-17 23:05:34.157423: step 180, loss = 3.11 (367.7 examples/sec; 0.348 sec/batch)
2017-10-17 23:05:37.612529: step 190, loss = 3.13 (373.4 examples/sec; 0.343 sec/batch)
2017-10-17 23:05:41.151749: step 200, loss = 3.01 (300.7 examples/sec; 0.426 sec/batch)
2017-10-17 23:05:44.646906: step 210, loss = 3.56 (370.0 examples/sec; 0.346 sec/batch)
2017-10-17 23:05:48.183005: step 220, loss = 3.26 (304.7 examples/sec; 0.420 sec/batch)
2017-10-17 23:05:51.635054: step 230, loss = 2.77 (367.6 examples/sec; 0.348 sec/batch)
2017-10-17 23:05:55.109216: step 240, loss = 2.70 (372.9 examples/sec; 0.343 sec/batch)
2017-10-17 23:05:58.660876: step 250, loss = 2.68 (363.8 examples/sec; 0.352 sec/batch)
2017-10-17 23:06:02.122376: step 260, loss = 2.62 (369.8 examples/sec; 0.346 sec/batch)
2017-10-17 23:06:05.670476: step 270, loss = 2.89 (369.6 examples/sec; 0.346 sec/batch)
2017-10-17 23:06:09.126694: step 280, loss = 2.70 (362.4 examples/sec; 0.353 sec/batch)
2017-10-17 23:06:12.654397: step 290, loss = 2.35 (372.7 examples/sec; 0.343 sec/batch)
2017-10-17 23:06:16.109538: step 300, loss = 2.44 (374.9 examples/sec; 0.341 sec/batch)
2017-10-17 23:06:19.656210: step 310, loss = 2.42 (369.0 examples/sec; 0.347 sec/batch)
2017-10-17 23:06:23.088560: step 320, loss = 2.41 (370.0 examples/sec; 0.346 sec/batch)
2017-10-17 23:06:26.648786: step 330, loss = 2.28 (368.1 examples/sec; 0.348 sec/batch)
2017-10-17 23:06:31.768565: step 340, loss = 2.63 (379.3 examples/sec; 0.337 sec/batch)
2017-10-17 23:06:35.287500: step 350, loss = 2.34 (362.5 examples/sec; 0.353 sec/batch)
2017-10-17 23:06:38.726879: step 360, loss = 2.17 (369.6 examples/sec; 0.346 sec/batch)
2017-10-17 23:06:42.268947: step 370, loss = 2.39 (364.9 examples/sec; 0.351 sec/batch)
2017-10-17 23:06:45.714981: step 380, loss = 2.00 (374.7 examples/sec; 0.342 sec/batch)
2017-10-17 23:06:49.228868: step 390, loss = 2.12 (367.7 examples/sec; 0.348 sec/batch)
2017-10-17 23:06:52.673184: step 400, loss = 2.04 (375.2 examples/sec; 0.341 sec/batch)
2017-10-17 23:06:56.183485: step 410, loss = 2.06 (376.2 examples/sec; 0.340 sec/batch)
2017-10-17 23:06:59.629171: step 420, loss = 1.77 (369.2 examples/sec; 0.347 sec/batch)
2017-10-17 23:07:03.143283: step 430, loss = 2.15 (370.0 examples/sec; 0.346 sec/batch)
2017-10-17 23:07:06.583372: step 440, loss = 2.24 (373.9 examples/sec; 0.342 sec/batch)
2017-10-17 23:07:10.084093: step 450, loss = 2.08 (374.8 examples/sec; 0.342 sec/batch)
2017-10-17 23:07:13.540338: step 460, loss = 1.92 (369.7 examples/sec; 0.346 sec/batch)
2017-10-17 23:07:17.056444: step 470, loss = 2.01 (367.8 examples/sec; 0.348 sec/batch)
2017-10-17 23:07:20.507319: step 480, loss = 1.72 (364.9 examples/sec; 0.351 sec/batch)
2017-10-17 23:07:24.010930: step 490, loss = 1.81 (370.3 examples/sec; 0.346 sec/batch)
2017-10-17 23:07:27.450471: step 500, loss = 2.03 (370.1 examples/sec; 0.346 sec/batch)
2017-10-17 23:07:32.562790: step 510, loss = 1.72 (372.8 examples/sec; 0.343 sec/batch)
2017-10-17 23:07:36.016536: step 520, loss = 1.97 (374.4 examples/sec; 0.342 sec/batch)
2017-10-17 23:07:39.538616: step 530, loss = 1.91 (365.5 examples/sec; 0.350 sec/batch)
2017-10-17 23:07:42.989583: step 540, loss = 1.87 (372.7 examples/sec; 0.343 sec/batch)
2017-10-17 23:07:46.525869: step 550, loss = 1.61 (363.9 examples/sec; 0.352 sec/batch)
2017-10-17 23:07:50.128859: step 560, loss = 1.67 (377.0 examples/sec; 0.340 sec/batch)
2017-10-17 23:07:53.646709: step 570, loss = 1.75 (372.2 examples/sec; 0.344 sec/batch)
2017-10-17 23:07:57.086924: step 580, loss = 1.71 (365.5 examples/sec; 0.350 sec/batch)
2017-10-17 23:08:00.600382: step 590, loss = 1.60 (374.1 examples/sec; 0.342 sec/batch)
2017-10-17 23:08:04.043435: step 600, loss = 1.89 (374.0 examples/sec; 0.342 sec/batch)
2017-10-17 23:08:07.548536: step 610, loss = 1.72 (309.3 examples/sec; 0.414 sec/batch)
2017-10-17 23:08:10.991961: step 620, loss = 1.52 (376.6 examples/sec; 0.340 sec/batch)
2017-10-17 23:08:14.452976: step 630, loss = 1.70 (367.7 examples/sec; 0.348 sec/batch)
2017-10-17 23:08:17.954659: step 640, loss = 1.68 (369.8 examples/sec; 0.346 sec/batch)
2017-10-17 23:08:21.476028: step 650, loss = 1.46 (372.8 examples/sec; 0.343 sec/batch)
2017-10-17 23:08:25.002300: step 660, loss = 1.89 (355.8 examples/sec; 0.360 sec/batch)
2017-10-17 23:08:30.094060: step 670, loss = 1.58 (367.7 examples/sec; 0.348 sec/batch)
2017-10-17 23:08:33.586550: step 680, loss = 1.48 (377.1 examples/sec; 0.339 sec/batch)
2017-10-17 23:08:37.024864: step 690, loss = 1.37 (374.0 examples/sec; 0.342 sec/batch)
2017-10-17 23:08:40.539023: step 700, loss = 1.94 (378.6 examples/sec; 0.338 sec/batch)
2017-10-17 23:08:43.968069: step 710, loss = 1.54 (373.8 examples/sec; 0.342 sec/batch)
2017-10-17 23:08:47.486152: step 720, loss = 1.36 (369.1 examples/sec; 0.347 sec/batch)
2017-10-17 23:08:50.929630: step 730, loss = 1.47 (374.6 examples/sec; 0.342 sec/batch)
2017-10-17 23:08:54.436907: step 740, loss = 1.34 (369.6 examples/sec; 0.346 sec/batch)
2017-10-17 23:08:57.886882: step 750, loss = 1.62 (373.0 examples/sec; 0.343 sec/batch)
2017-10-17 23:09:01.405794: step 760, loss = 1.69 (367.6 examples/sec; 0.348 sec/batch)
2017-10-17 23:09:04.856747: step 770, loss = 1.52 (374.7 examples/sec; 0.342 sec/batch)
2017-10-17 23:09:08.378450: step 780, loss = 1.23 (357.4 examples/sec; 0.358 sec/batch)
2017-10-17 23:09:11.815037: step 790, loss = 1.28 (372.1 examples/sec; 0.344 sec/batch)
2017-10-17 23:09:15.332228: step 800, loss = 1.31 (372.7 examples/sec; 0.343 sec/batch)
2017-10-17 23:09:18.753148: step 810, loss = 1.41 (371.8 examples/sec; 0.344 sec/batch)
2017-10-17 23:09:22.284257: step 820, loss = 1.23 (359.7 examples/sec; 0.356 sec/batch)
2017-10-17 23:09:25.895077: step 830, loss = 1.40 (375.9 examples/sec; 0.341 sec/batch)
2017-10-17 23:09:31.143438: step 840, loss = 1.42 (374.7 examples/sec; 0.342 sec/batch)
2017-10-17 23:09:34.574942: step 850, loss = 1.14 (373.1 examples/sec; 0.343 sec/batch)
2017-10-17 23:09:38.101646: step 860, loss = 1.24 (371.6 examples/sec; 0.344 sec/batch)
2017-10-17 23:09:41.538664: step 870, loss = 1.29 (366.8 examples/sec; 0.349 sec/batch)
2017-10-17 23:09:45.066945: step 880, loss = 1.11 (371.8 examples/sec; 0.344 sec/batch)
2017-10-17 23:09:48.510143: step 890, loss = 1.38 (374.7 examples/sec; 0.342 sec/batch)
2017-10-17 23:09:52.022410: step 900, loss = 1.16 (378.7 examples/sec; 0.338 sec/batch)
2017-10-17 23:09:55.466224: step 910, loss = 1.35 (372.5 examples/sec; 0.344 sec/batch)
2017-10-17 23:09:58.982531: step 920, loss = 1.21 (369.9 examples/sec; 0.346 sec/batch)
2017-10-17 23:10:02.449913: step 930, loss = 1.26 (369.5 examples/sec; 0.346 sec/batch)
2017-10-17 23:10:05.988063: step 940, loss = 1.24 (371.4 examples/sec; 0.345 sec/batch)
2017-10-17 23:10:09.449135: step 950, loss = 1.66 (363.4 examples/sec; 0.352 sec/batch)
2017-10-17 23:10:12.964026: step 960, loss = 1.37 (378.2 examples/sec; 0.338 sec/batch)
2017-10-17 23:10:16.633537: step 970, loss = 1.14 (353.3 examples/sec; 0.362 sec/batch)
2017-10-17 23:10:20.128395: step 980, loss = 1.35 (368.4 examples/sec; 0.347 sec/batch)
2017-10-17 23:10:23.583184: step 990, loss = 1.22 (372.6 examples/sec; 0.344 sec/batch)
2017-10-17 23:10:27.100925: step 1000, loss = 1.26 (307.9 examples/sec; 0.416 sec/batch)
2017-10-17 23:10:32.194592: step 1010, loss = 1.98 (372.9 examples/sec; 0.343 sec/batch)
2017-10-17 23:10:35.681884: step 1020, loss = 1.35 (370.9 examples/sec; 0.345 sec/batch)
2017-10-17 23:10:39.107717: step 1030, loss = 1.47 (372.9 examples/sec; 0.343 sec/batch)
2017-10-17 23:10:42.617100: step 1040, loss = 1.26 (375.5 examples/sec; 0.341 sec/batch)
2017-10-17 23:10:46.039359: step 1050, loss = 1.10 (375.6 examples/sec; 0.341 sec/batch)
2017-10-17 23:10:49.545019: step 1060, loss = 1.21 (368.6 examples/sec; 0.347 sec/batch)
2017-10-17 23:10:52.978497: step 1070, loss = 1.26 (375.0 examples/sec; 0.341 sec/batch)
2017-10-17 23:10:56.483828: step 1080, loss = 1.48 (308.1 examples/sec; 0.415 sec/batch)
2017-10-17 23:10:59.925343: step 1090, loss = 1.39 (373.5 examples/sec; 0.343 sec/batch)
2017-10-17 23:11:03.356042: step 1100, loss = 1.25 (374.3 examples/sec; 0.342 sec/batch)
2017-10-17 23:11:06.874431: step 1110, loss = 1.23 (373.8 examples/sec; 0.342 sec/batch)
2017-10-17 23:11:10.337412: step 1120, loss = 1.22 (363.2 examples/sec; 0.352 sec/batch)
2017-10-17 23:11:13.869157: step 1130, loss = 1.24 (371.2 examples/sec; 0.345 sec/batch)
2017-10-17 23:11:17.297121: step 1140, loss = 1.41 (371.0 examples/sec; 0.345 sec/batch)
2017-10-17 23:11:20.979572: step 1150, loss = 1.21 (374.8 examples/sec; 0.342 sec/batch)
2017-10-17 23:11:24.412092: step 1160, loss = 1.17 (369.5 examples/sec; 0.346 sec/batch)
2017-10-17 23:11:27.928682: step 1170, loss = 1.24 (375.2 examples/sec; 0.341 sec/batch)
2017-10-17 23:11:33.037680: step 1180, loss = 1.32 (375.7 examples/sec; 0.341 sec/batch)
2017-10-17 23:11:36.536466: step 1190, loss = 1.09 (377.0 examples/sec; 0.340 sec/batch)
2017-10-17 23:11:39.967021: step 1200, loss = 1.15 (371.1 examples/sec; 0.345 sec/batch)
2017-10-17 23:11:43.470161: step 1210, loss = 1.30 (375.1 examples/sec; 0.341 sec/batch)
2017-10-17 23:11:46.911216: step 1220, loss = 1.20 (371.8 examples/sec; 0.344 sec/batch)
2017-10-17 23:11:50.415417: step 1230, loss = 1.17 (372.3 examples/sec; 0.344 sec/batch)
2017-10-17 23:11:53.844839: step 1240, loss = 1.28 (373.4 examples/sec; 0.343 sec/batch)
2017-10-17 23:11:57.366370: step 1250, loss = 1.18 (369.5 examples/sec; 0.346 sec/batch)
2017-10-17 23:12:00.796118: step 1260, loss = 1.13 (374.7 examples/sec; 0.342 sec/batch)
2017-10-17 23:12:04.291483: step 1270, loss = 1.10 (375.9 examples/sec; 0.341 sec/batch)
2017-10-17 23:12:07.745439: step 1280, loss = 1.06 (371.5 examples/sec; 0.345 sec/batch)
2017-10-17 23:12:11.252587: step 1290, loss = 1.22 (367.6 examples/sec; 0.348 sec/batch)
2017-10-17 23:12:14.683971: step 1300, loss = 1.07 (368.5 examples/sec; 0.347 sec/batch)
2017-10-17 23:12:18.184756: step 1310, loss = 1.02 (375.2 examples/sec; 0.341 sec/batch)
2017-10-17 23:12:21.604660: step 1320, loss = 1.34 (378.4 examples/sec; 0.338 sec/batch)
2017-10-17 23:12:25.184328: step 1330, loss = 1.07 (367.5 examples/sec; 0.348 sec/batch)
2017-10-17 23:12:30.356966: step 1340, loss = 1.18 (62.0 examples/sec; 2.065 sec/batch)
2017-10-17 23:12:33.847489: step 1350, loss = 1.17 (375.6 examples/sec; 0.341 sec/batch)
2017-10-17 23:12:37.279363: step 1360, loss = 1.08 (371.2 examples/sec; 0.345 sec/batch)
2017-10-17 23:12:40.780412: step 1370, loss = 1.03 (376.9 examples/sec; 0.340 sec/batch)
2017-10-17 23:12:44.206537: step 1380, loss = 1.05 (375.4 examples/sec; 0.341 sec/batch)
2017-10-17 23:12:47.691657: step 1390, loss = 1.05 (371.2 examples/sec; 0.345 sec/batch)
2017-10-17 23:12:51.118686: step 1400, loss = 0.87 (372.6 examples/sec; 0.344 sec/batch)
2017-10-17 23:12:54.630239: step 1410, loss = 1.17 (371.2 examples/sec; 0.345 sec/batch)
2017-10-17 23:12:58.063857: step 1420, loss = 1.31 (372.5 examples/sec; 0.344 sec/batch)
2017-10-17 23:13:01.571029: step 1430, loss = 0.91 (366.5 examples/sec; 0.349 sec/batch)
2017-10-17 23:13:05.014028: step 1440, loss = 1.03 (376.8 examples/sec; 0.340 sec/batch)
2017-10-17 23:13:08.518832: step 1450, loss = 1.09 (376.4 examples/sec; 0.340 sec/batch)
2017-10-17 23:13:11.946655: step 1460, loss = 1.54 (373.2 examples/sec; 0.343 sec/batch)
2017-10-17 23:13:15.450191: step 1470, loss = 1.19 (364.2 examples/sec; 0.351 sec/batch)
2017-10-17 23:13:18.871530: step 1480, loss = 1.12 (374.8 examples/sec; 0.342 sec/batch)
2017-10-17 23:13:22.380388: step 1490, loss = 1.00 (308.9 examples/sec; 0.414 sec/batch)
2017-10-17 23:13:25.825579: step 1500, loss = 1.20 (375.1 examples/sec; 0.341 sec/batch)
2017-10-17 23:13:31.041753: step 1510, loss = 0.93 (309.6 examples/sec; 0.413 sec/batch)
2017-10-17 23:13:34.479200: step 1520, loss = 1.01 (375.5 examples/sec; 0.341 sec/batch)
2017-10-17 23:13:37.925161: step 1530, loss = 1.27 (368.3 examples/sec; 0.348 sec/batch)
2017-10-17 23:13:41.422049: step 1540, loss = 1.12 (370.2 examples/sec; 0.346 sec/batch)
2017-10-17 23:13:44.845194: step 1550, loss = 0.99 (371.5 examples/sec; 0.345 sec/batch)
2017-10-17 23:13:48.338670: step 1560, loss = 1.00 (372.3 examples/sec; 0.344 sec/batch)
2017-10-17 23:13:51.773351: step 1570, loss = 1.22 (360.0 examples/sec; 0.356 sec/batch)
2017-10-17 23:13:55.288808: step 1580, loss = 1.07 (360.3 examples/sec; 0.355 sec/batch)
2017-10-17 23:13:58.711575: step 1590, loss = 1.02 (376.0 examples/sec; 0.340 sec/batch)
2017-10-17 23:14:02.188672: step 1600, loss = 1.28 (371.7 examples/sec; 0.344 sec/batch)
2017-10-17 23:14:05.623696: step 1610, loss = 1.02 (373.5 examples/sec; 0.343 sec/batch)
2017-10-17 23:14:09.120059: step 1620, loss = 1.25 (370.1 examples/sec; 0.346 sec/batch)
2017-10-17 23:14:12.549867: step 1630, loss = 1.18 (375.2 examples/sec; 0.341 sec/batch)
2017-10-17 23:14:16.064121: step 1640, loss = 1.31 (379.5 examples/sec; 0.337 sec/batch)
2017-10-17 23:14:19.485638: step 1650, loss = 1.13 (378.1 examples/sec; 0.339 sec/batch)
2017-10-17 23:14:22.981746: step 1660, loss = 0.98 (377.0 examples/sec; 0.340 sec/batch)
2017-10-17 23:14:26.496453: step 1670, loss = 2.20 (369.4 examples/sec; 0.347 sec/batch)
2017-10-17 23:14:31.712535: step 1680, loss = 1.17 (368.3 examples/sec; 0.348 sec/batch)
2017-10-17 23:14:35.142173: step 1690, loss = 1.03 (374.2 examples/sec; 0.342 sec/batch)
2017-10-17 23:14:38.625156: step 1700, loss = 1.43 (382.6 examples/sec; 0.335 sec/batch)
2017-10-17 23:14:42.051122: step 1710, loss = 1.18 (370.9 examples/sec; 0.345 sec/batch)
2017-10-17 23:14:45.536369: step 1720, loss = 1.05 (379.0 examples/sec; 0.338 sec/batch)
2017-10-17 23:14:48.946842: step 1730, loss = 1.15 (372.8 examples/sec; 0.343 sec/batch)
2017-10-17 23:14:52.463896: step 1740, loss = 0.90 (368.5 examples/sec; 0.347 sec/batch)
2017-10-17 23:14:55.911431: step 1750, loss = 1.09 (375.6 examples/sec; 0.341 sec/batch)
2017-10-17 23:14:59.415869: step 1760, loss = 1.44 (372.7 examples/sec; 0.343 sec/batch)
2017-10-17 23:15:02.856537: step 1770, loss = 1.05 (376.3 examples/sec; 0.340 sec/batch)
2017-10-17 23:15:06.368719: step 1780, loss = 0.96 (371.5 examples/sec; 0.345 sec/batch)
2017-10-17 23:15:09.796146: step 1790, loss = 0.97 (379.1 examples/sec; 0.338 sec/batch)
2017-10-17 23:15:13.304082: step 1800, loss = 0.93 (360.6 examples/sec; 0.355 sec/batch)
2017-10-17 23:15:16.716921: step 1810, loss = 1.03 (380.7 examples/sec; 0.336 sec/batch)
2017-10-17 23:15:20.205411: step 1820, loss = 1.13 (372.1 examples/sec; 0.344 sec/batch)
2017-10-17 23:15:23.630949: step 1830, loss = 1.29 (374.4 examples/sec; 0.342 sec/batch)
2017-10-17 23:15:27.148856: step 1840, loss = 0.99 (307.3 examples/sec; 0.416 sec/batch)
2017-10-17 23:15:32.424825: step 1850, loss = 0.87 (368.2 examples/sec; 0.348 sec/batch)
2017-10-17 23:15:35.938854: step 1860, loss = 1.16 (376.7 examples/sec; 0.340 sec/batch)
2017-10-17 23:15:39.379455: step 1870, loss = 1.09 (365.7 examples/sec; 0.350 sec/batch)
2017-10-17 23:15:42.890851: step 1880, loss = 1.07 (372.3 examples/sec; 0.344 sec/batch)
2017-10-17 23:15:46.323524: step 1890, loss = 0.90 (373.7 examples/sec; 0.343 sec/batch)
2017-10-17 23:15:49.833399: step 1900, loss = 1.08 (371.9 examples/sec; 0.344 sec/batch)
2017-10-17 23:15:53.278525: step 1910, loss = 1.03 (369.0 examples/sec; 0.347 sec/batch)
2017-10-17 23:15:56.795843: step 1920, loss = 0.94 (367.2 examples/sec; 0.349 sec/batch)
2017-10-17 23:16:00.232221: step 1930, loss = 1.02 (368.4 examples/sec; 0.347 sec/batch)
2017-10-17 23:16:03.727157: step 1940, loss = 1.58 (309.8 examples/sec; 0.413 sec/batch)
2017-10-17 23:16:07.166379: step 1950, loss = 0.97 (373.0 examples/sec; 0.343 sec/batch)
2017-10-17 23:16:10.611398: step 1960, loss = 1.10 (374.2 examples/sec; 0.342 sec/batch)
2017-10-17 23:16:14.118947: step 1970, loss = 1.11 (374.3 examples/sec; 0.342 sec/batch)
2017-10-17 23:16:17.555423: step 1980, loss = 1.08 (374.1 examples/sec; 0.342 sec/batch)
2017-10-17 23:16:21.125973: step 1990, loss = 1.05 (371.9 examples/sec; 0.344 sec/batch)
2017-10-17 23:16:24.564927: step 2000, loss = 1.22 (372.5 examples/sec; 0.344 sec/batch)
2017-10-17 23:16:28.072368: step 2010, loss = 1.38 (370.3 examples/sec; 0.346 sec/batch)
2017-10-17 23:16:33.308108: step 2020, loss = 0.94 (371.9 examples/sec; 0.344 sec/batch)
2017-10-17 23:16:36.748558: step 2030, loss = 0.95 (371.8 examples/sec; 0.344 sec/batch)
2017-10-17 23:16:40.259328: step 2040, loss = 0.96 (366.6 examples/sec; 0.349 sec/batch)
2017-10-17 23:16:43.682206: step 2050, loss = 1.23 (374.6 examples/sec; 0.342 sec/batch)
2017-10-17 23:16:47.210585: step 2060, loss = 0.96 (310.7 examples/sec; 0.412 sec/batch)
2017-10-17 23:16:50.755327: step 2070, loss = 1.14 (374.6 examples/sec; 0.342 sec/batch)
2017-10-17 23:16:54.252980: step 2080, loss = 1.21 (308.2 examples/sec; 0.415 sec/batch)
2017-10-17 23:16:57.680517: step 2090, loss = 1.01 (377.1 examples/sec; 0.339 sec/batch)
2017-10-17 23:17:01.123144: step 2100, loss = 0.99 (371.9 examples/sec; 0.344 sec/batch)
2017-10-17 23:17:04.643523: step 2110, loss = 1.10 (361.8 examples/sec; 0.354 sec/batch)
2017-10-17 23:17:08.063922: step 2120, loss = 1.07 (374.9 examples/sec; 0.341 sec/batch)
2017-10-17 23:17:11.569491: step 2130, loss = 1.07 (367.7 examples/sec; 0.348 sec/batch)
2017-10-17 23:17:15.029636: step 2140, loss = 1.25 (362.9 examples/sec; 0.353 sec/batch)
2017-10-17 23:17:18.540071: step 2150, loss = 0.97 (375.6 examples/sec; 0.341 sec/batch)
2017-10-17 23:17:21.967760: step 2160, loss = 1.17 (371.0 examples/sec; 0.345 sec/batch)
2017-10-17 23:17:25.480669: step 2170, loss = 1.12 (365.8 examples/sec; 0.350 sec/batch)
2017-10-17 23:17:28.915441: step 2180, loss = 1.18 (379.0 examples/sec; 0.338 sec/batch)
2017-10-17 23:17:34.142324: step 2190, loss = 1.07 (374.9 examples/sec; 0.341 sec/batch)
2017-10-17 23:17:37.564122: step 2200, loss = 1.28 (374.9 examples/sec; 0.341 sec/batch)
2017-10-17 23:17:41.066514: step 2210, loss = 1.09 (377.2 examples/sec; 0.339 sec/batch)
2017-10-17 23:17:44.507188: step 2220, loss = 1.06 (372.0 examples/sec; 0.344 sec/batch)
2017-10-17 23:17:48.017007: step 2230, loss = 0.98 (378.0 examples/sec; 0.339 sec/batch)
2017-10-17 23:17:51.470655: step 2240, loss = 1.28 (368.7 examples/sec; 0.347 sec/batch)
2017-10-17 23:17:54.972536: step 2250, loss = 1.03 (373.8 examples/sec; 0.342 sec/batch)
2017-10-17 23:17:58.411408: step 2260, loss = 0.98 (370.4 examples/sec; 0.346 sec/batch)
2017-10-17 23:18:01.927971: step 2270, loss = 0.87 (368.6 examples/sec; 0.347 sec/batch)
2017-10-17 23:18:05.376189: step 2280, loss = 0.87 (375.6 examples/sec; 0.341 sec/batch)
2017-10-17 23:18:08.906404: step 2290, loss = 0.88 (365.6 examples/sec; 0.350 sec/batch)
2017-10-17 23:18:12.357321: step 2300, loss = 1.07 (366.3 examples/sec; 0.349 sec/batch)
2017-10-17 23:18:15.866777: step 2310, loss = 1.06 (367.9 examples/sec; 0.348 sec/batch)
2017-10-17 23:18:19.314590: step 2320, loss = 0.80 (371.9 examples/sec; 0.344 sec/batch)
2017-10-17 23:18:22.818434: step 2330, loss = 0.90 (368.4 examples/sec; 0.347 sec/batch)
2017-10-17 23:18:26.262179: step 2340, loss = 0.82 (365.6 examples/sec; 0.350 sec/batch)
2017-10-17 23:18:31.574491: step 2350, loss = 1.04 (365.7 examples/sec; 0.350 sec/batch)
2017-10-17 23:18:35.025099: step 2360, loss = 1.23 (372.1 examples/sec; 0.344 sec/batch)
2017-10-17 23:18:38.522010: step 2370, loss = 0.99 (375.9 examples/sec; 0.340 sec/batch)
2017-10-17 23:18:41.966170: step 2380, loss = 1.02 (372.1 examples/sec; 0.344 sec/batch)
2017-10-17 23:18:45.461225: step 2390, loss = 1.24 (374.7 examples/sec; 0.342 sec/batch)
2017-10-17 23:18:48.927253: step 2400, loss = 0.89 (374.7 examples/sec; 0.342 sec/batch)
2017-10-17 23:18:52.413112: step 2410, loss = 0.95 (372.1 examples/sec; 0.344 sec/batch)
2017-10-17 23:18:55.843463: step 2420, loss = 1.03 (373.8 examples/sec; 0.342 sec/batch)
2017-10-17 23:18:59.361977: step 2430, loss = 0.98 (359.5 examples/sec; 0.356 sec/batch)
2017-10-17 23:19:02.797089: step 2440, loss = 1.06 (375.3 examples/sec; 0.341 sec/batch)
2017-10-17 23:19:06.306259: step 2450, loss = 0.99 (362.7 examples/sec; 0.353 sec/batch)
2017-10-17 23:19:09.757274: step 2460, loss = 1.01 (369.8 examples/sec; 0.346 sec/batch)
2017-10-17 23:19:13.288688: step 2470, loss = 0.92 (369.4 examples/sec; 0.347 sec/batch)
2017-10-17 23:19:16.731978: step 2480, loss = 0.89 (376.0 examples/sec; 0.340 sec/batch)
2017-10-17 23:19:20.244332: step 2490, loss = 1.04 (373.8 examples/sec; 0.342 sec/batch)
2017-10-17 23:19:23.677676: step 2500, loss = 0.91 (373.8 examples/sec; 0.342 sec/batch)
2017-10-17 23:19:27.190640: step 2510, loss = 0.95 (368.4 examples/sec; 0.347 sec/batch)
2017-10-17 23:19:32.315792: step 2520, loss = 0.86 (367.5 examples/sec; 0.348 sec/batch)
2017-10-17 23:19:35.820850: step 2530, loss = 3.32 (376.6 examples/sec; 0.340 sec/batch)
2017-10-17 23:19:39.241460: step 2540, loss = 1.46 (369.6 examples/sec; 0.346 sec/batch)
2017-10-17 23:19:42.730073: step 2550, loss = 1.38 (375.2 examples/sec; 0.341 sec/batch)
2017-10-17 23:19:46.160905: step 2560, loss = 1.07 (371.4 examples/sec; 0.345 sec/batch)
2017-10-17 23:19:49.679059: step 2570, loss = 1.34 (375.1 examples/sec; 0.341 sec/batch)
2017-10-17 23:19:53.114419: step 2580, loss = 1.25 (368.3 examples/sec; 0.348 sec/batch)
2017-10-17 23:19:56.620702: step 2590, loss = 1.02 (368.9 examples/sec; 0.347 sec/batch)
2017-10-17 23:20:00.067552: step 2600, loss = 1.25 (372.1 examples/sec; 0.344 sec/batch)
2017-10-17 23:20:03.567483: step 2610, loss = 1.05 (376.4 examples/sec; 0.340 sec/batch)
2017-10-17 23:20:07.002714: step 2620, loss = 1.36 (374.0 examples/sec; 0.342 sec/batch)
2017-10-17 23:20:10.506474: step 2630, loss = 1.22 (371.7 examples/sec; 0.344 sec/batch)
2017-10-17 23:20:13.928897: step 2640, loss = 1.10 (376.4 examples/sec; 0.340 sec/batch)
2017-10-17 23:20:17.426827: step 2650, loss = 0.95 (311.9 examples/sec; 0.410 sec/batch)
2017-10-17 23:20:20.937623: step 2660, loss = 0.89 (369.7 examples/sec; 0.346 sec/batch)
2017-10-17 23:20:24.443148: step 2670, loss = 1.32 (311.9 examples/sec; 0.410 sec/batch)
2017-10-17 23:20:27.864197: step 2680, loss = 1.01 (379.0 examples/sec; 0.338 sec/batch)
2017-10-17 23:20:33.141828: step 2690, loss = 1.29 (368.7 examples/sec; 0.347 sec/batch)
2017-10-17 23:20:36.588243: step 2700, loss = 1.02 (371.0 examples/sec; 0.345 sec/batch)
2017-10-17 23:20:40.065246: step 2710, loss = 1.30 (374.3 examples/sec; 0.342 sec/batch)
2017-10-17 23:20:43.523792: step 2720, loss = 1.11 (372.5 examples/sec; 0.344 sec/batch)
2017-10-17 23:20:47.035661: step 2730, loss = 1.14 (308.3 examples/sec; 0.415 sec/batch)
2017-10-17 23:20:50.490009: step 2740, loss = 0.98 (371.2 examples/sec; 0.345 sec/batch)
2017-10-17 23:20:53.950754: step 2750, loss = 1.13 (369.1 examples/sec; 0.347 sec/batch)
2017-10-17 23:20:57.461429: step 2760, loss = 1.02 (369.5 examples/sec; 0.346 sec/batch)
2017-10-17 23:21:00.891625: step 2770, loss = 0.96 (375.6 examples/sec; 0.341 sec/batch)
2017-10-17 23:21:04.430452: step 2780, loss = 1.18 (363.0 examples/sec; 0.353 sec/batch)
2017-10-17 23:21:07.863096: step 2790, loss = 1.02 (377.8 examples/sec; 0.339 sec/batch)
2017-10-17 23:21:11.360142: step 2800, loss = 1.09 (365.0 examples/sec; 0.351 sec/batch)
2017-10-17 23:21:14.834568: step 2810, loss = 1.06 (374.0 examples/sec; 0.342 sec/batch)
2017-10-17 23:21:18.343267: step 2820, loss = 0.87 (368.0 examples/sec; 0.348 sec/batch)
2017-10-17 23:21:21.798265: step 2830, loss = 0.94 (372.9 examples/sec; 0.343 sec/batch)
2017-10-17 23:21:25.328323: step 2840, loss = 0.85 (368.9 examples/sec; 0.347 sec/batch)
2017-10-17 23:21:28.778656: step 2850, loss = 1.06 (374.9 examples/sec; 0.341 sec/batch)
2017-10-17 23:21:33.922044: step 2860, loss = 2.50 (370.7 examples/sec; 0.345 sec/batch)
2017-10-17 23:21:37.427999: step 2870, loss = 28321233216159537168862443250647040.00 (358.5 examples/sec; 0.357 sec/batch)
Traceback (most recent call last):
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1139, in _do_call
    return fn(*args)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1121, in _run_fn
    status, run_metadata)
  File "/usr/lib/python3.5/contextlib.py", line 66, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
	 [[Node: gradients/conv1/BiasAdd_grad/tuple/control_dependency_1_S361 = _Recv[client_terminated=false, recv_device="/job:ps/replica:0/task:2/cpu:0", send_device="/job:worker/replica:0/task:0/cpu:0", send_device_incarnation=-9123273934494448932, tensor_name="edge_764_gradients/conv1/BiasAdd_grad/tuple/control_dependency_1", tensor_type=DT_FLOAT, _device="/job:ps/replica:0/task:2/cpu:0"]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/myEFSvolume/deeplearning-cfn/examples/tensorflow/cifar10_multi_machine_train.py", line 118, in <module>
    tf.app.run()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/myEFSvolume/deeplearning-cfn/examples/tensorflow/cifar10_multi_machine_train.py", line 113, in main
    mon_sess.run(train_op)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py", line 505, in run
    run_metadata=run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py", line 842, in run
    run_metadata=run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py", line 798, in run
    return self._sess.run(*args, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py", line 952, in run
    run_metadata=run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py", line 798, in run
    return self._sess.run(*args, **kwargs)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 789, in run
    run_metadata_ptr)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 997, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1132, in _do_run
    target_list, options, run_metadata)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
	 [[Node: gradients/conv1/BiasAdd_grad/tuple/control_dependency_1_S361 = _Recv[client_terminated=false, recv_device="/job:ps/replica:0/task:2/cpu:0", send_device="/job:worker/replica:0/task:0/cpu:0", send_device_incarnation=-9123273934494448932, tensor_name="edge_764_gradients/conv1/BiasAdd_grad/tuple/control_dependency_1", tensor_type=DT_FLOAT, _device="/job:ps/replica:0/task:2/cpu:0"]()]]

Caused by op 'conv2/conv2/conv2/activations', defined at:
  File "/myEFSvolume/deeplearning-cfn/examples/tensorflow/cifar10_multi_machine_train.py", line 118, in <module>
    tf.app.run()
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File "/myEFSvolume/deeplearning-cfn/examples/tensorflow/cifar10_multi_machine_train.py", line 87, in main
    logits = cifar10.inference(images)
  File "/myEFSvolume/deeplearning-cfn/examples/tensorflow/models/tutorials/image/cifar10/cifar10.py", line 231, in inference
    _activation_summary(conv2)
  File "/myEFSvolume/deeplearning-cfn/examples/tensorflow/models/tutorials/image/cifar10/cifar10.py", line 93, in _activation_summary
    tf.summary.histogram(tensor_name + '/activations', x)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/summary/summary.py", line 221, in histogram
    tag=scope.rstrip('/'), values=values, name=scope)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_logging_ops.py", line 131, in _histogram_summary
    name=name)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py", line 767, in apply_op
    op_def=op_def)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py", line 1269, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Nan in summary histogram for: conv2/conv2/conv2/activations
	 [[Node: conv2/conv2/conv2/activations = HistogramSummary[T=DT_FLOAT, _device="/job:worker/replica:0/task:0/cpu:0"](conv2/conv2/conv2/activations/tag, conv2/conv2)]]
	 [[Node: gradients/conv1/BiasAdd_grad/tuple/control_dependency_1_S361 = _Recv[client_terminated=false, recv_device="/job:ps/replica:0/task:2/cpu:0", send_device="/job:worker/replica:0/task:0/cpu:0", send_device_incarnation=-9123273934494448932, tensor_name="edge_764_gradients/conv1/BiasAdd_grad/tuple/control_dependency_1", tensor_type=DT_FLOAT, _device="/job:ps/replica:0/task:2/cpu:0"]()]]

